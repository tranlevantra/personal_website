[
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Welcome to my Blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nI photoshoped my Testamur\n\n\n\n\n\n\n\n\n\n\n\nDecember 18, 2024\n\n\nTra Tran\n\n\n1 min\n\n\n\n\n\n\n\n\n\n\n\n\nOn model performance assessment in binary classification tasks\n\n\n\n\n\nA reflection on the often-neglected aspects of model performance assessment in binary classification tasks\n\n\n\n\n\nDecember 11, 2024\n\n\nTra Tran\n\n\n6 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "home/index.html",
    "href": "home/index.html",
    "title": "Tra Tran",
    "section": "",
    "text": "I‚Äôm Tr√† ‚Äî originally from Vietnam, now living in Australia. Aside from /Chah/, I also go by Tea or April.\n‚òï Tea means Tr√† in Vietnamese,\nüåº April is my English name ‚Äî chosen for my birth month, and because it just felt right."
  },
  {
    "objectID": "home/index.html#education",
    "href": "home/index.html#education",
    "title": "Tra Tran",
    "section": "üéì Education",
    "text": "üéì Education\nRMIT University ‚Äî Melbourne, Australia\nMaster of Statistics & Operations Research\nMarch 2023 ‚Äì December 2024\nRMIT University Vietnam ‚Äî Hanoi, Vietnam\nBachelor of Logistics & Supply Chain Management\nOctober 2017 ‚Äì May 2022"
  },
  {
    "objectID": "blog/posts/2024-12-11 decision threshold/index.html",
    "href": "blog/posts/2024-12-11 decision threshold/index.html",
    "title": "On model performance assessment in binary classification tasks",
    "section": "",
    "text": "Thank you, Asma Ali and Thomas Bui for your insightful feedback"
  },
  {
    "objectID": "blog/posts/2024-12-11 decision threshold/index.html#before-we-start",
    "href": "blog/posts/2024-12-11 decision threshold/index.html#before-we-start",
    "title": "On model performance assessment in binary classification tasks",
    "section": "Before We Start",
    "text": "Before We Start\nThis article is aimed at those with some background in Machine Learning as it assumes the reader is familiar with the concepts of binary classification, model training, and evaluation metrics. Some refreshment is provided below.\n\n\n\n\n\n\nQuick Refresher: Model Performance Assessment in Binary Classification Tasks\n\n\n\n\n\nBinary classification is a supervised learning task where the goal is to classify observations into one of two classes (0 or 1). For instance, in a medical diagnosis task, the model is trained to predict whether a patient has a disease (1) or not (0).\nThere are two types of training algorithms for binary classification:\n\nProbability based algorithms (logistic regression, support vector machines with a sigmoid kernel, neural networks with a sigmoid activation function, etc.) which output probability - a value between 0 and 1 indicating the likelihood of an observation belonging to the positive class (1)\nAnd Decision based algorithms (tree-based algorithms, etc.) which output a class label directly.\n\nIn reality, probability outputs are preferred because they provide more information about the uncertainty of the prediction. Nevertheless, decision based algorithms can output probabilities by using calibration methods when required.\nAfter training, the model is evaluated on a test dataset to assess its performance. The evaluation process typically involves converting the predicted probabilities into class labels using a decision threshold, and then calculating various performance metrics based on the confusion matrix.\nIn details,\nDecision Threshold: is a value between 0 and 1 such that if the probability output is greater than or equal to the decision threshold, the observation is classified as the positive class (1); otherwise, it is classified as the negative class (0).\nConfusion matrix: is obtained by comparing the predicted class labels from the model with the actual class labels (0 or 1) in the test dataset. It shows the counts of:\n\nTrue Positive (TP): The number of positive (1) instances correctly predicted as positive (1).\nTrue Negative (TN): The number of negative (0) instances correctly predicted as negative (0).\nFalse Positive (FP): The number of negative (0) instances incorrectly predicted as positive (1).\nFalse Negative (FN): The number of positive (1) instances incorrectly predicted as negative (0)."
  },
  {
    "objectID": "blog/posts/2024-12-11 decision threshold/index.html#unpopular-opinions-on-binary-classification-assessment",
    "href": "blog/posts/2024-12-11 decision threshold/index.html#unpopular-opinions-on-binary-classification-assessment",
    "title": "On model performance assessment in binary classification tasks",
    "section": "Unpopular Opinions on Binary Classification Assessment",
    "text": "Unpopular Opinions on Binary Classification Assessment\n\nCalibration is an often-neglected aspect when assessing the performance of classification models\nAccuracy is just one of many evaluation metrics ‚Äî a high accuracy alone doesn‚Äôt guarantee good real-world performance, especially in imbalanced datasets.\nWhile ROC curves are helpful for sensitivity analysis and threshold selection, interpreting them isn‚Äôt always straightforward, particularly when misclassification costs are unequal.\n\n\nCalibration metrics\nCalibration metrics in classification tasks evaluate how close the model‚Äôs predicted probabilities are to the true class labels (0 or 1), indicating the model‚Äôs reliability in estimating probabilities.\nFor example, in healthcare applications, a well-calibrated model would assign probabilities that closely reflect the true likelihood of disease presence or absence.\nSome of the most common calibration metrics are:\n\nThe mean squared difference between predicted probabilities and actual class labels.\n\\[\n\\text{Brier Score} = \\frac{1}{n} \\sum_{i=1}^{n} (P_i - o_i)^2\n\\]\nThe negative log-likelihood of the predicted probabilities given the actual class labels.\n\\[\n\\text{Log Loss} = -\\frac{1}{n} \\sum_{i=1}^{n} \\left[ o_i \\log(P_i) + (1 - o_i) \\log(1 - P_i) \\right]\n\\]\nThe root mean squared difference between predicted probabilities and actual class labels.\n\\[\n\\text{RMS Calibration Error} = \\sqrt{ \\frac{1}{n} \\sum_{i=1}^{n} (P_i - o_i)^2 }\n\\]\n\nWhile the above metrics are useful for evaluating a model‚Äôs overall calibration, they don‚Äôt reveal how the model performs on each class individually. That‚Äôs why visualising predicted probabilities against actual class labels ‚Äî as done in a reliability diagram (example in Figure 1 below)‚Äî is essential for uncovering insights that metrics alone might miss.\n\n\n\nFigure 1: The negative class (0) shows good calibration, with predicted probabilities tightly clustered around the actual class label. In contrast, the positive class (1) shows poorer calibration, with greater variability\n\n\n\n\nAccuracy Metrics\nThese metrics are calculated after converting predicted probabilities to class labels (0 or 1) using the decision threshold.\n\nAccuracy: \\(\\frac{TP + TN}{TP + TN + FP + FN}\\)\nSensitivity (True Positive Rate, Recall): \\(\\frac{TP}{TP + FN}\\)\nSpecificity (True Negative Rate): \\(\\frac{TN}{TN + FP}\\)\nPrecision (Positive Predictive Value): \\(\\frac{TP}{TP + FP}\\)\nFalse Positive Rate (FPR): \\(\\frac{FP}{FP + TN}\\)\nF1 Score: \\(2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}\\)\nBalanced Accuracy: \\(\\frac{\\text{Sensitivity} + \\text{Specificity}}{2}\\)\nAnd many more ‚Ä¶\n\nThese metrics give a fuller picture than accuracy alone, especially in imbalanced datasets where the number of instances in one class outweighs the other.\nFor example, in a breast cancer classification dataset where 90% of the instances are cancer-free (class 0) and only 10% have cancer (class 1), a model that predicts everyone to be cancer-free would still achieve 90% accuracy. However, such a model would be useless, as it fails to identify any actual cancer cases.\n\n\n\nFigure 2: An example of model performance on test data at the commonly used threshold of 0.5. The model achieves 90% accuracy but fails to detect the single positive case.\n\n\n\n\nSensitivity analysis\nROC Curve plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) at various thresholds to reveal how sensitive the model is to changes in the decision threshold for each class.\n\n\n\nFigure 3: This ROC curve suggests the model performs poorly across most thresholds. It only identifies positives at very low thresholds, with a high false positive rate. Overall, the model struggles to separate the classes effectively\n\n\nThe optimal decision threshold is often chosen near the top-left corner of the ROC curve, where sensitivity and specificity are balanced. However, this assumes equal costs for false positives and false negatives ‚Äî which is rarely the case.\nIn breast cancer screening, for example, missing a cancer case (false negative) is more serious than flagging a healthy person (false positive). In such cases, the threshold is lowered to prioritize detecting positives, making ROC interpretation less straightforward.\nFigure 4 shows the same model as in Figure 3, with reliability diagram and a confusion matrix that updates with the threshold ‚Äî making it easier to see how model‚Äôs performance changes.\n\n\n\nFigure 4: From the reliability diagram, varying the threshold dynamically reveals how lowering it reduce false negatives while increase false positives. The confusion matrix further highlights how sensitive the two classes are to changes in threshold"
  },
  {
    "objectID": "blog/posts/2024-12-11 decision threshold/index.html#some-notes",
    "href": "blog/posts/2024-12-11 decision threshold/index.html#some-notes",
    "title": "On model performance assessment in binary classification tasks",
    "section": "Some Notes",
    "text": "Some Notes\nThis article is not meant to be an exhaustive guide to model performance assessment in binary classification, but rather a reflection on often overlooked aspects ‚Äî particularly the role of calibration metrics and sensitivity to decision thresholds.\nThe idea of plotting predicted probabilities against actual class labels to create a reliability diagram stems from my own experimentation. All example visualisations are created using R and the ggplot2 package, plot data and code to create these figures are documented here"
  },
  {
    "objectID": "blog/posts/2024-12-18 testamur/index.html",
    "href": "blog/posts/2024-12-18 testamur/index.html",
    "title": "I photoshoped my Testamur",
    "section": "",
    "text": "Long story short: the name on my testamur originally appeared as ‚ÄúTran Le Van Tra‚Äù ‚Äî a version adjusted to English naming conventions.\nBut my parents don‚Äôt speak English, and my name might be the only thing they recognize on that piece of paper. So I photoshoped it ‚Äî to Tr·∫ßn L√™ V√¢n Tr√†.\nTo honor them. For all the sacrifices they made so I could be here today.\n\n\n\nA little tribute to where I came from ‚Äî and who I did this for.\n\n\nAnd‚Ä¶ to flex a little, too üòå\nSome say ‚ÄúMasters is just delayed unemployment‚Äù, but to me, it‚Äôs a privilege to study, to learn, and to grow.\nWith a bachelor‚Äôs degree in business and no prior training in advanced mathematics, the biggest achievement of these past two years has been learning math from the ground up and now, diving into advanced concepts in statistics with confidence.\nYes, I‚Äôve been nerdy. I‚Äôve spent most of these two years studying. But that, too, is a kind of employment."
  },
  {
    "objectID": "project/index.html",
    "href": "project/index.html",
    "title": "My Project Listings",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\nindex\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about/index.html",
    "href": "about/index.html",
    "title": "Tra Tran",
    "section": "",
    "text": "I was born and raised in Northwestern Vietnam ‚Äî the mountainous region of quiet beauty and the most kind-hearted people.\n\n\nI like to do math, to cook, and to code.\n(Yes, in that order.)"
  },
  {
    "objectID": "about/index.html#lets-get-to-know-me-better",
    "href": "about/index.html#lets-get-to-know-me-better",
    "title": "Tra Tran",
    "section": "",
    "text": "I was born and raised in Northwestern Vietnam ‚Äî the mountainous region of quiet beauty and the most kind-hearted people.\n\n\nI like to do math, to cook, and to code.\n(Yes, in that order.)"
  },
  {
    "objectID": "about/index.html#math",
    "href": "about/index.html#math",
    "title": "Tra Tran",
    "section": "üß† MATH",
    "text": "üß† MATH\nI‚Äôm a nerd ‚Äî no doubt about it. But I have an explanation. Some people hit the gym to stay fit; I started solving math problems to keep my brain agile and sharp. And it‚Äôs funny ‚Äî the more you do it, the more you want to do it. That‚Äôs how I ended up here.\nThe kind of math I study is statistics ‚Äî not pure math, but a mathematical science nonetheless. I feel incredibly grateful to study math at this level. And as clich√© as it sounds: the more I learn, the more I realize how little I actually know.\n\n\n\nüí° A key result in probability theory underpinning many important statistical applications that deserves more attention\n\n\nBecause AI and machine learning are trending ‚Äî and statistics is their backbone ‚Äî here are some resources on the math of data science (tested and approved by a nerd ü§ì):\n\nüìê Calculus\n\nBook: Calculus by James Stewart\nCourse: MIT OpenCourseWare\n\nSingle Variable Calculus\n\nMultivariable Calculus\n\n\nA fun, intuitive approach: Khan Academy\n\nIntegral Calculus\n\nDifferential Calculus\n\nMultivariable Calculus\n\n\n\n\nüìä Linear Algebra\n\nBook: Linear Algebra with Applications by Steven J. Leon\nCourse: MIT OCW 18.06 Linear Algebra\n\nVisual & intuitive: Math for Game Devs by Freya Holmer ‚Äî she‚Äôs so cute, love her üòç\n\n\n\nüìà Mathematical Statistics\nI absolutely adore üìò Statistical Inference by Casella and Berger, and I‚Äôve set my sights next on üìó Probability and Measure by Patrick Billingsley next.\nAttached links are free content available on the internet. But if you want to support the authors, please consider buying the books!üòâ"
  },
  {
    "objectID": "about/index.html#cooking",
    "href": "about/index.html#cooking",
    "title": "Tra Tran",
    "section": "üçú COOKING",
    "text": "üçú COOKING\nMy cooking journey began a long time ago. My parents work incredibly hard to provide for us, and as the oldest sister, I naturally found myself in the kitchen early on.\nThings really leveled up when I moved to Australia ‚Äî I began cooking exquisite Vietnamese dishes to share my culture with new friends, soothe my homesickness, and explore cuisines from around the world.\nMy inspiration? This lovely collection from VnExpress. It‚Äôs in Vietnamese, but three things transcend all language barriers ‚Äî love, music, and yes‚Ä¶ food! ‚ù§Ô∏èüé∂üç≤"
  },
  {
    "objectID": "about/index.html#coding",
    "href": "about/index.html#coding",
    "title": "Tra Tran",
    "section": "üíª CODING",
    "text": "üíª CODING\nI‚Äôm still new to coding ‚Äî and feel so lucky that auto-completion, Copilot, and ChatGPT were already around when I began.\nMy favorite language so far is Java ‚Äî it‚Äôs declarative, easy to read, and has just the right amount of strictness to catch bugs early. But the language I‚Äôm most fluent in is R, because I study Statistics ‚Äî and R was developed by statisticians, for statisticians üìà.\nArguing about which programming language is ‚Äúbetter‚Äù feels a bit pointless to me. Languages are tools, and each serves a different purpose. What matters is what you build with them.\nI do wish I‚Äôd taken Data Structures and Algorithms in school ‚Äî It‚Äôs all good, there‚Äôs an abundance of amazing online resources now. üìöüíª"
  },
  {
    "objectID": "home/index.html#hello",
    "href": "home/index.html#hello",
    "title": "Tra Tran",
    "section": "",
    "text": "I‚Äôm Tr√† ‚Äî originally from Vietnam, now living in Australia. Aside from /Chah/, I also go by Tea or April.\n‚òï Tea means Tr√† in Vietnamese,\nüåº April is my English name ‚Äî chosen for my birth month, and because it just felt right."
  }
]